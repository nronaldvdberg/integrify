{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b38b056-164e-49ce-b562-37dcad5836bf",
   "metadata": {},
   "source": [
    "# Function to communicate with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2098e5ca-1c7d-42ef-a557-72efa8fb89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_openai_response(tweets, temperature=0.3, max_tokens=4000):\n",
    "    # set api key, which is partly stored in a text file\n",
    "    with open('C:/openaiapikey.txt', 'r') as file:\n",
    "        open_ai_api_key = file.readline().strip() + \"9izgJtucT6\"    \n",
    "\n",
    "    # build system prompt\n",
    "    system_prompt =  \"I'm an AI trained to classify and rate the radical intent in social media posts.\\n\"\n",
    "    system_prompt += \"Given a set of tweets, I provide an analysis for each and rate the radical intent of the writer on a scale from 0 (no radical intent) to 100 (extreme radical intent).\\n\"\n",
    "    system_prompt += \"It's important to note that news reports, even about violent events, are not necessarily an indicator of radical intent of the writer.\\n\"\n",
    "    system_prompt += \"Here are some examples of how I do this:\\n\"\n",
    "    \n",
    "    system_prompt += \"Example 1:\\n\"\n",
    "    system_prompt += \"Tweet 23: 'Kill all Americans, they are pigs and must all die'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 23 advocates violence and harm against a particular group of people, which is a key indicator of radical intent.\\n\"\n",
    "    system_prompt += \"Radicalness: 100\\n\"\n",
    "    \n",
    "    system_prompt += \"Example 2:\\n\"\n",
    "    system_prompt += \"Tweet 48: 'я ненавижу всех европейцев'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 48 translates to 'I hate all Europeans'; it does not directly advocate for violence, but it does express a strong negative sentiment towards a broad group of people, which can be seen as a mild form of radical intent.\\n\"\n",
    "    system_prompt += \"Radicalness: 50\\n\"\n",
    "    \n",
    "    system_prompt += \"Example 3:\\n\"\n",
    "    system_prompt += \"Tweet 88: 'breakingnews car bomb targeted military personnel near military barracks istanbul turkey pkk move twitterkurds'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 88 includes violent content, but its tone and presentation are similar to a news headline, and it seems to be meant to inform rather than to radicalize.\\n\"\n",
    "    system_prompt += \"Radicalness: 5\\n\"\n",
    "    \n",
    "    system_prompt += \"Example 4:\\n\"\n",
    "    system_prompt += \"Tweet 832: 'isis claims responsibility ied attack west algeria resulted killing two soldiers wounding others'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 832 looks like a news report, indicating very low radical intent.\\n\"\n",
    "    system_prompt += \"Radicalness: 5\\n\"\n",
    "    \n",
    "    system_prompt += \"Example 5:\\n\"\n",
    "    system_prompt += \"Tweet: 'rt amaqagency islamic state fighters advance deirezzor city captured regimes last pos sina ah last month'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 5 reports on the Islamic State's advances, similar to a news headline. Although it deals with violent events, it does not advocate for violence or express radical intent, but merely reports events.\\n\"\n",
    "    system_prompt += \"Radicalness: 10\\n\"\n",
    "\n",
    "    system_prompt += \"Example 6:\\n\"\n",
    "    system_prompt += \"Tweet 12365: 'al battar engl leading destruction entirely killing wounding crew may allah praised caliphate news'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 12365 advocates for the destruction and killing of a group of people, and praises the actions of a particular group, indicating a high level of radical intent.\\n\"\n",
    "    system_prompt += \"Radicalness: 90\\n\"\n",
    "\n",
    "    system_prompt += \"Example 7:\\n\"\n",
    "    system_prompt += \"Tweet 12366: 'trump kill muslims bullets dipped pig blood video https co'\\n\"\n",
    "    system_prompt += \"Analysis: Tweet 12366 advocates for violence against a specific religious group, using derogatory language, indicating a high level of radical intent.\\n\"\n",
    "    system_prompt += \"Radicalness: 95\\n\"\n",
    "        \n",
    "    # build user prompt\n",
    "    user_prompt = \"Now, analyze the \" + str(len(tweets)) + \" tweets below. Response with two lines per tweet, starting with 'Analysis:' and 'Radicalness:'.\\n\"\n",
    "    for tweet in tweets:\n",
    "        user_prompt += tweet + \"\\n\"\n",
    "    \n",
    "    # build the message array for the request\n",
    "    msg_array = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # perform the request\n",
    "    openai.api_key = open_ai_api_key\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=msg_array,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    # process the response\n",
    "    token_cnt = response['usage']['total_tokens']\n",
    "    content = response['choices'][0]['message']['content'].replace(\"\\n\",\"\")\n",
    "    parts = content.split(\"Analysis:\")\n",
    "    return_data = []\n",
    "    for part in parts:\n",
    "        subparts = part.split(\"Radicalness:\")\n",
    "        if len(subparts) == 2:\n",
    "            tweet_nr = subparts[0].strip().split(' ')[1].strip()\n",
    "            tweet_analysis = \" \".join(subparts[0].strip().split(' ')[2:])\n",
    "            tweet_radicalness = subparts[1].strip()\n",
    "            return_data.append([tweet_nr, tweet_radicalness, tweet_analysis])\n",
    "    \n",
    "    # return\n",
    "    return return_data, token_cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6ef98-b716-4b08-90b7-aa376a2bdc30",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f967ba8-f974-4ff0-bb54-2145473fb8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the first 20 tweets, because we already have results for them\n",
      "Going to process 3286 tweets in batches of 10\n",
      "== processed 9 tweets so far; total cost = 1166 tokens ($0.00)\n",
      "== processed 19 tweets so far; total cost = 2313 tokens ($0.00)\n",
      "== processed 29 tweets so far; total cost = 3551 tokens ($0.01)\n",
      "== processed 39 tweets so far; total cost = 4690 tokens ($0.01)\n",
      "== processed 49 tweets so far; total cost = 5791 tokens ($0.01)\n",
      "== processed 59 tweets so far; total cost = 6924 tokens ($0.01)\n",
      "== processed 69 tweets so far; total cost = 8067 tokens ($0.02)\n",
      "== processed 79 tweets so far; total cost = 9227 tokens ($0.02)\n",
      "== processed 89 tweets so far; total cost = 10410 tokens ($0.02)\n",
      "== processed 99 tweets so far; total cost = 11646 tokens ($0.02)\n",
      "== processed 109 tweets so far; total cost = 12764 tokens ($0.03)\n",
      "== processed 119 tweets so far; total cost = 13874 tokens ($0.03)\n",
      "== processed 129 tweets so far; total cost = 15140 tokens ($0.03)\n",
      "== processed 139 tweets so far; total cost = 16388 tokens ($0.03)\n",
      "== processed 149 tweets so far; total cost = 17519 tokens ($0.04)\n",
      "== processed 159 tweets so far; total cost = 18749 tokens ($0.04)\n",
      "== processed 169 tweets so far; total cost = 19848 tokens ($0.04)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e965f0a7ac29f13cc6f07ed79b25edcc in your message.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25956\\532177195.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf\"Tweet {tweet_number}: '{tweets_dict[tweet_number]}'\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet_subset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# get openai response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_openai_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;31m# loop over the results and add them to the excel file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mtoken_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_cnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25956\\2460805895.py\u001b[0m in \u001b[0;36mget_openai_response\u001b[1;34m(tweets, temperature, max_tokens)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# perform the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_ai_api_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         )\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             return (\n\u001b[1;32m--> 624\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e965f0a7ac29f13cc6f07ed79b25edcc in your message.)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# set the maximum nr of tweets to process in this run (for testing)\n",
    "max_cnt = 3305\n",
    "batch_size = 15\n",
    "\n",
    "# read the input .csv\n",
    "filename_in = 'Twitter Group3.csv'\n",
    "filename_out = 'twitter_group3.xlsx'\n",
    "tweets_dict = {}\n",
    "cnt = 0\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(filename_out):\n",
    "    df = pd.read_excel(filename_out)\n",
    "    processed_tweets = df['tweet nr'].values.tolist()\n",
    "    print(f\"Skipping the first {len(processed_tweets)} tweets, because we already have results for them\")\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['rater', 'tweet nr', 'tweet', 'label', 'AI score', 'AI analysis'])\n",
    "    processed_tweets = []\n",
    "\n",
    "with open(filename_in, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if cnt < max_cnt:\n",
    "            tweet_number = int(row[''])\n",
    "            if tweet_number not in processed_tweets:\n",
    "                tweet = row['tweets']\n",
    "                tweets_dict[tweet_number] = tweet\n",
    "                cnt += 1\n",
    "\n",
    "# print info about the upcoming processing\n",
    "print(f\"Going to process {len(tweets_dict)} tweets in batches of {batch_size}\")\n",
    "\n",
    "# set who is going to rate which rows (the numbers specify the first and last tweet nr for each rater)\n",
    "rater_dict = {'eleazar' : ( 9920, 10581),\n",
    "              'md abd'   : (10582, 11242),\n",
    "              'md ari'   : (11243, 11903),\n",
    "              'pramod'  : (11904, 12564),\n",
    "              'ronald'  : (12565, 13225)}\n",
    "\n",
    "# loop over the tweets and feed them to OpenAI\n",
    "tweet_numbers = list(tweets_dict.keys())\n",
    "token_sum = 0\n",
    "for i in range(0, len(tweet_numbers), batch_size):\n",
    "    # build list with the next batch of tweets (or all remaining ones if fewer than 20 left)\n",
    "    tweet_subset = tweet_numbers[i:i+batch_size]\n",
    "    tweets = [f\"Tweet {tweet_number}: '{tweets_dict[tweet_number]}'\" for tweet_number in tweet_subset]\n",
    "    # get openai response\n",
    "    results, token_cnt = get_openai_response(tweets, temperature=0.5, max_tokens=2000)\n",
    "    # loop over the results and add them to the excel file    \n",
    "    token_sum += int(token_cnt)\n",
    "    for tweet_nr in tweet_subset:\n",
    "        # find the AI result for this tweet in the list of results\n",
    "        result = [tup for tup in results if int(tup[0]) == int(tweet_nr)]\n",
    "        if result == []:\n",
    "            result = [tweet_nr, -1, 'N/A']\n",
    "        else:\n",
    "            result = result[0]\n",
    "        # make sure the score is numeric (convert things like 'N/A' to -1)\n",
    "        try:\n",
    "            result[1] = float(result[1])\n",
    "        except ValueError:\n",
    "            result[1] = -1\n",
    "        # assign rater based on tweet number\n",
    "        rater = next((r for r, (start, end) in rater_dict.items() if start <= tweet_nr <= end), 'unknown')\n",
    "        # add a line to the DataFrame and save\n",
    "        new_row = pd.DataFrame({'rater': [rater], 'tweet nr': [tweet_nr], 'tweet': [tweets_dict[tweet_nr]], 'label': [''], 'AI score': [result[1]], 'AI analysis': [result[2]]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(filename_out, index=False)\n",
    "        # display tweets that were rated as high radicalness\n",
    "        if int(result[1]) >= 65:\n",
    "            print(f\"\\nT = {tweets_dict[tweet_nr]}\")\n",
    "            print(f\"A = {result[2]}\")\n",
    "            print(f\"R = {result[1]}\")    \n",
    "    print(f\"=> processed {i+batch_size} tweets so far; total cost = {token_sum} tokens (${(token_sum/1000)*0.002:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
