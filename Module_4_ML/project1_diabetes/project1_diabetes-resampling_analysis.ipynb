{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaa9975-ec07-4cae-9a5b-6d05a52b25ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A look into how SMOTE-ENN can artificially boost model accuracy\n",
    "### By Ronald van den Berg\n",
    "This notebook demonstrates how classifier performance on the diabetes dataset can be artificially boosted by applying SMOTE-ENN to the data *before* splitting them into test and train sets.\n",
    "\n",
    "As explained below, the boost is the result of a poor methodological choice, not because of having a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24751a3a-72fa-4223-a72f-36a7b0a81868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference to the data\n",
    "data_file = './data/diabetes_binary_health_indicators_BRFSS2015.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57dafd-ca16-4685-948a-1754eaebfccb",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47d441f-f723-4416-b54b-766b7ad5ca0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22fe53-f646-49a8-88ae-0bd0d74f3d10",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fadb81-b174-41df-9dea-4f27e35e167a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('./data/diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "\n",
    "# log-transform the BMI column as its distribution looks log-normalish\n",
    "df['BMI_log'] = np.log(df['BMI'])\n",
    "df = df.drop('BMI', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002f341-0b1e-44e9-8a36-ec073386c397",
   "metadata": {},
   "source": [
    "# Define features (X), target (y), model, and resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd767d9-c268-4b6f-8e2e-16eff4291d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and target\n",
    "X = df.drop('Diabetes_binary', axis=1)\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# define model and its parameter space\n",
    "model = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=20230629))\n",
    "param_space = {\n",
    "    'gradientboostingclassifier__n_estimators': [100, 200, 300],\n",
    "    'gradientboostingclassifier__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'gradientboostingclassifier__max_depth': [3, 5, 10, 20]\n",
    "}\n",
    "    \n",
    "# create the optimizer object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_space, n_iter=36, cv=5, n_jobs=6, scoring='accuracy', random_state=20230629)\n",
    "\n",
    "# create the resampler object\n",
    "resampler = SMOTEENN(sampling_strategy = 'all', random_state = 20230630)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79729-f02e-4a75-b604-873f50091cc7",
   "metadata": {},
   "source": [
    "# Analysis 1: Split -> SMOTE-ENN -> Fit -> Evaluate\n",
    "This is the common order of doing things. It is proper, because the test set is an unmodified subset of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d0a7d5-8180-4aff-84f6-08bcfd14c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting...\n",
      "resampling...\n",
      "fitting...\n",
      "accuracy = 0.780\n"
     ]
    }
   ],
   "source": [
    "# split the raw data\n",
    "print('splitting...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20230629)\n",
    "\n",
    "# resample using SMOTE-ENN (applied to training set only)\n",
    "print('resampling...')\n",
    "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# fit\n",
    "print('fitting...')\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# evaluate\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc40ed-7679-4bad-86b5-b42f2951c31b",
   "metadata": {},
   "source": [
    "# Analysis 2: SMOTE-ENN -> Split -> Fit -> Evaluate\n",
    "This order is unusual and problematic, because the test data are tinkered with - that can create biases in the evaluation (for more on this, see *Final Thoughts* below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bac787-1d99-48b7-96e6-0c4289ca78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling...\n",
      "splitting...\n",
      "fitting...\n",
      "accuracy = 0.962\n"
     ]
    }
   ],
   "source": [
    "# resample raw data\n",
    "print('resampling...')\n",
    "Xr, yr = resampler.fit_resample(X, y)\n",
    "\n",
    "# split resampled data\n",
    "print('splitting...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xr, yr, test_size=0.2, random_state=20230629)\n",
    "\n",
    "# fit\n",
    "print('fitting...')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c15d2-8ef2-408e-af18-497a023c6c20",
   "metadata": {},
   "source": [
    "# Intermediate conclusions \n",
    "Applying SMOTE-ENN resampling before splitting the data gives an enormous accuracy boost compared to applying resampling to only the training data (78.0% -> 96.2%).\n",
    "\n",
    "I think that the boost comes from the ENN part of SMOTE-ENN: \n",
    "\n",
    "\"*ENN (Edited Nearest Neighbor) is a cleaning technique that removes any instance of the majority class whose predicted class by the k-NN method contradicts the actual class of the instance*\" (source: GPT-4)\n",
    "\n",
    "In other words: **ENN removes difficult-to-classify cases from the data**. By doing this before splitting the data into test and train sets, we prevent difficult cases from entering the test data. \n",
    "\n",
    "That leads to a boost in accuracy, but in my view this is a *cheat*: the superior accuracy is not due to having a superior model or a superior training method, but due to having made the test set less challenging. The high accuracy will *not* generalize to the real-world actual data, which contains many challengings cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfebace-0888-428f-84b5-7b8e5406d88d",
   "metadata": {},
   "source": [
    "## Appendix: does ENN perhaps even *hurt* classifier training?\n",
    "If ENN removes difficult cases from the dataset, then the classifier will never be exposit might actually hurt model training, because it will only learn how to deal with the easier cases. \n",
    "\n",
    "To test this, we rerun the above analysis with SMOTE resampling. It's identical to SMOTE-ENN, except that it doesn't remove challenging cases from the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b68871f-8d10-49cf-a936-c735c877c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (split -> resample) = 0.863\n",
      "accuracy (resample -> split) = 0.919\n"
     ]
    }
   ],
   "source": [
    "# redefine the resampler\n",
    "resampler = SMOTE(sampling_strategy = 'all', random_state = 20230630)\n",
    "\n",
    "# rerun analysis #1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20230629)\n",
    "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy (split -> resample) = {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# rerun analysis #2\n",
    "Xr, yr = resampler.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xr, yr, test_size=0.2, random_state=20230629)\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy (resample -> split) = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56c256-7bf6-4ca2-8bff-d5a5c3c4659d",
   "metadata": {},
   "source": [
    "## Summary & Final Thoughts\n",
    "Here is a summary of all results:\n",
    "\n",
    "| Approach | Classifier | Resampling Method | Resampling target | Accuracy | Notes |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 1 | Gradient boosting | SMOTE-ENN | Training data only | 78.0% | The ENN step harms model training (cf. Approach 3)|\n",
    "| 2 | Gradient boosting | SMOTE-ENN | Train + test data | 96.2% | Accuracy boost due to cheating |\n",
    "| 3 | Gradient boosting | SMOTE | Training data only | 86.3% | Best approach out of these four |\n",
    "| 4 | Gradient boosting | SMOTE | Train + test data | 91.9% | Accuracy boost due to cheating |\n",
    "\n",
    "**Approaches 2** and **4** give superior accuracy, but that's because of fiddling with the test data. This will not generalize to real data and, therefore, these approaches should probably be avoided.\n",
    "\n",
    "**Approach 3** gives 86.3% performance. This is similar to the best performance that I got when playing around with other classifiers (including KNN, Random Forest, XGboost, etc) and class balacing methods (oversampling, class weights). Hence, I believe that 86% is close to the maximum accuracy on this data set.\n",
    "\n",
    "Interestingly, when we comparing **Approach 1** to Approach 3, we see that the ENN step even *hurts* performance. This is because the classifier is not exposed to difficult cases during training. As a result, it can only classify the relatively easy cases when evaluating the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585bc8a-fdee-4cf6-989d-df27e8493bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
