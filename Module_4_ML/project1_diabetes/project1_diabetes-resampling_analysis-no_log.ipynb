{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaa9975-ec07-4cae-9a5b-6d05a52b25ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ronald's analysis of how SMOTE-ENN can be used to artificially boost model accuracy\n",
    "This notebook demonstrates how classifier performance on the diabetes dataset can be artificially boosted by applying SMOTE-ENN to the data *before* splitting them into test and train sets.\n",
    "\n",
    "An explanation of the boost is provided in *Final Thoughts* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24751a3a-72fa-4223-a72f-36a7b0a81868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference to the data\n",
    "data_file = './data/diabetes_binary_health_indicators_BRFSS2015.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57dafd-ca16-4685-948a-1754eaebfccb",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47d441f-f723-4416-b54b-766b7ad5ca0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22fe53-f646-49a8-88ae-0bd0d74f3d10",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fadb81-b174-41df-9dea-4f27e35e167a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('./data/diabetes_binary_health_indicators_BRFSS2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca8a39-e091-4239-89f0-cd21382d46cc",
   "metadata": {},
   "source": [
    "# Drop uninformative features to speed up the rest of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e3c361-0eed-4324-b0b1-65664f8c8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop uninformative features to speed things up a bit\n",
    "drop_cols = ['CholCheck', 'AnyHealthcare', 'HvyAlcoholConsump', 'Stroke', 'NoDocbcCost', 'DiffWalk', 'HeartDiseaseorAttack', 'Veggies', 'PhysActivity', 'Sex', 'Fruits']\n",
    "for col in drop_cols:\n",
    "    df = df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002f341-0b1e-44e9-8a36-ec073386c397",
   "metadata": {},
   "source": [
    "# Define features (X), target (y), model, and resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd767d9-c268-4b6f-8e2e-16eff4291d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features and target\n",
    "X = df.drop('Diabetes_binary', axis=1)\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# define model and its parameter space\n",
    "model = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=20230629))\n",
    "param_space = {\n",
    "    'gradientboostingclassifier__n_estimators': [100, 200, 300],\n",
    "    'gradientboostingclassifier__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'gradientboostingclassifier__max_depth': [3, 5, 10, 20]\n",
    "}\n",
    "    \n",
    "# create the optimizer object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_space, n_iter=36, cv=5, n_jobs=6, scoring='accuracy', random_state=20230629)\n",
    "\n",
    "# create the resampler object\n",
    "resampler = SMOTEENN(sampling_strategy = 'all', random_state = 20230630)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79729-f02e-4a75-b604-873f50091cc7",
   "metadata": {},
   "source": [
    "# Analysis 1: Split -> SMOTE-ENN -> Fit -> Evaluate\n",
    "This is the common order of doing things. It is proper, because the test set is an unmodified subset of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d0a7d5-8180-4aff-84f6-08bcfd14c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting...\n",
      "resampling...\n",
      "fitting...\n",
      "accuracy = 0.773\n"
     ]
    }
   ],
   "source": [
    "# split the raw data\n",
    "print('splitting...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20230629)\n",
    "\n",
    "# resample using SMOTE-ENN (applied to training set only)\n",
    "print('resampling...')\n",
    "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# fit\n",
    "print('fitting...')\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# evaluate\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc40ed-7679-4bad-86b5-b42f2951c31b",
   "metadata": {},
   "source": [
    "# Analysis 2: SMOTE-ENN -> Split -> Fit -> Evaluate\n",
    "This order is unusual and problematic, because the test data are tinkered with - that can create biases in the evaluation (for more on this, see *Final Thoughts* below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48bac787-1d99-48b7-96e6-0c4289ca78b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling...\n",
      "splitting...\n",
      "fitting...\n",
      "accuracy = 0.963\n"
     ]
    }
   ],
   "source": [
    "# resample raw data\n",
    "print('resampling...')\n",
    "Xr, yr = resampler.fit_resample(X, y)\n",
    "\n",
    "# split resampled data\n",
    "print('splitting...')\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xr, yr, test_size=0.2, random_state=20230629)\n",
    "\n",
    "# fit\n",
    "print('fitting...')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c15d2-8ef2-408e-af18-497a023c6c20",
   "metadata": {},
   "source": [
    "# Final Thoughts \n",
    "I think that the accuracy boost comes from the ENN part of SMOTE-ENN: \n",
    "\n",
    "\"*ENN (Edited Nearest Neighbor) is a cleaning technique that removes any instance of the majority class whose predicted class by the k-NN method contradicts the actual class of the instance*\" (source: GPT-4)\n",
    "\n",
    "By applying ENN *before* splitting the data, we will have removed many of the difficult cases from the dataset before we create the test set. As a consequence, the test set will be much less challenging, leading to higher model accuracy. \n",
    "\n",
    "However, this seems to be a bad practice, because the accuracy increase is not due a better trained model, but rather because of the removal of difficult-to-classify instances from the test data. This tinkering with the test set biases the model evaluation process, making it seem as if the model is performing better than it actually would on unseen, real-world data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfebace-0888-428f-84b5-7b8e5406d88d",
   "metadata": {},
   "source": [
    "## Appendix: rerun the above with SMOTE instead of SMOTE-ENN\n",
    "If the accuracy boost is really due to ENN (as I hypothesized above), then the accuracy difference should be much smaller when using SMOTE. To test this, I reran the above analysis with SMOTE instead of SMOTE-ENN. \n",
    "\n",
    "As can be seen below, the accuracy boost due to oversampling *before* splitting then indeed becomes smaller, but there still is a boost.\n",
    "\n",
    "This is because there is still a problem of *data leakage*: the synthetic data in the training set are partly based on the samples of the test set. Hence, the training set contains information about the test set, which artificially increases model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b68871f-8d10-49cf-a936-c735c877c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (split -> resample) = 0.856\n",
      "accuracy (resample -> split) = 0.909\n"
     ]
    }
   ],
   "source": [
    "# redefine the resampler\n",
    "resampler = SMOTE(sampling_strategy = 'all', random_state = 20230630)\n",
    "\n",
    "# rerun analysis #1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20230629)\n",
    "X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy (split -> resample) = {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# rerun analysis #2\n",
    "Xr, yr = resampler.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xr, yr, test_size=0.2, random_state=20230629)\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(f\"accuracy (resample -> split) = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
